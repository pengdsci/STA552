---
title: 'Data Science Demystified'
author: "Cheng Peng"
date: "West Chester University"
output:
  pdf_document:
    toc: yes
    toc_depth: '4'
  html_document:
    toc: yes
    toc_float: yes
    toc_collapsed: yes
    toc_depth: 4
    number_sections: yes
    code_folding: hide
    code_download: yes
    theme: lumen
---

```{=html}
<style type="text/css">
/* Table of content - navigation */
div#TOC li {
    list-style:none;
    background-color:lightgray;
    background-image:none;
    background-repeat:none;
    background-position:0;
    font-family: Arial, Helvetica, sans-serif;
    color: #780c0c;
}


/* Title fonts */
h1.title {
  font-size: 24px;
  font-weight: bold;
  color: darkblue;
  text-align: center;
  font-family: Arial, Helvetica, sans-serif;
  font-variant-caps: normal;
}
h4.author { 
  font-size: 16px;
  font-weight: bold;
  font-family: Arial, Helvetica, sans-serif;
  color: navy;
  text-align: center;
}
h4.date { 
  font-size: 16px;
  font-weight: bold;
  font-family: Arial, Helvetica, sans-serif;
  color: darkblue;
  text-align: center;
}

/* Section headers */
h1 {
    font-size: 22px;
  font-weight: bold;
    font-family: "Times New Roman", Times, serif;
    color: darkred;
    text-align: left;
}

h2 {
    font-size: 18px;
  font-weight: bold;
    font-family: "Times New Roman", Times, serif;
    color: navy;
    text-align: left;
}

h3 { 
    font-size: 15px;
  font-weight: bold;
    font-family: "Times New Roman", Times, serif;
    color: darkred;
    text-align: left;
}

h4 {
    font-size: 18px;
  font-weight: bold;
    font-family: "Times New Roman", Times, serif;
    color: darkred;
    text-align: left;
}

/* Decoration of hyperlinks  */

/* unvisited link */
a:link {
  color: green;
}

/* visited link */
a:visited {
  color: purple;
}

/* mouse over link */
a:hover {
  color: red;
}

/* selected link */
a:active {
  color: yellow;
}


code, pre {
  font-family: "Fira Code", "Consolas", "Courier New", monospace;
  font-size: 14px;
  background-color: #f5f5f5;
  padding: 5px;
  border-radius: 5px;
}

  p0 {
     background-color: purple;
     color: gold;
     text-align: center;
     font-size: 16px;
     font-weight:bold
     text-shadow: 2px 2px red;
   } 

</style>
```

```{r setup, include=FALSE}
if (!require("tidyverse")) {
   install.packages("tidyverse")
   library(tidyverse)
}
if (!require("knitr")) {
   install.packages("knitr")
   library(knitr)
}
if (!require("gifski")) {
   install.packages("gifski")
   library(gifski)
}
if (!require("plotly")) {
   install.packages("plotly")
   library(plotly)
}
# knitr::opts_knit$set(root.dir = "C:\\STA490\\w05")
knitr::opts_chunk$set(echo = TRUE,       
                      warning = FALSE,   
                      result = TRUE,   
                      message = FALSE,
                      comment = NA)
```


\

# Introduction

The debate over the exact definition of **data science** has evolved significantly since the term was first introduced, reflecting rapid advancements in technology and shifting industry needs. This ongoing discussion underscores the field's dynamic nature and the challenges in establishing clear boundaries. Despite these debates, **data science** is broadly understood as an interdisciplinary field that integrates **statistical analysis**, **computational methods**, **machine learning algorithms**, and **domain expertise** to extract meaningful insights from data. The multidisciplinary nature of data science, combined with the ambiguous title and inconsistent roles of **data scientist**, has led to widespread misperceptions in the neighboring fields that data science is equivalent to statistics or computer science. 


This note aims to provide a broad perspective on the emerging field of data science by tracing its foundational roots in classical statistics, computer science, domain expertise, and advanced visual analytics.

Through a series of comparative analyses—examining focus areas, foundational principles, problem-solving approaches, applications, and more—this note tries to clarify common misconceptions about data science in relation to its associated disciplines and subfields. Additionally, it emphasizes the advancements brought by emerging technologies and the evolving skill sets now in demand across industries.

\

# A Brief History

The evolution of data science spans decades, marked by foundational innovations, technological advancements, and paradigm shifts in data analysis. Below are the key milestones that shaped the field:

**Early Foundations (1960s–1970s)**

* 1962: *John Tukey’*s paper *The Future of Data Analysis* predicted the merger of statistics and computing, laying the groundwork for data science.
* 1964: *Karen Sp&auml;rck Jones* advanced natural language processing with her work on semantic classification.
* 1974: *Peter Naur* coined the term **data science* in *Concise Survey of Computer Methods*, **defining it as the science of handling data for practical applications**.
* 1977:
  + **The International Association for Statistical Computing (IASC)** formed to bridge statistical methodology with computer technology.
  + *Tukey* published **Exploratory Data Analysis**, emphasizing hypothesis generation through data.

**Emergence of Computational Methods (1980s–1990s)**

* 1986: *Geoffrey Hinton*’s work on backpropagation for neural networks revolutionized machine learning.
* 1989: The first Knowledge Discovery in Databases (KDD) workshop laid the foundation for data mining.
* 1993: *Yoshua Bengio* founded Mila, a leading AI research institute.
* 1997: IBM’s Deep Blue defeated chess champion Garry Kasparov, showcasing AI’s potential.
* 1999: *Jacob Zahavi* highlighted the need for tools to manage growing data volumes, foreshadowing big data challenges.

**Big Data Revolution (2000s–2010s)**

* 2001: *William S. Cleveland*’s **Data Science: An Action Plan** proposed expanding statistics into computational domains.
* 2005: Hadoop emerged to process massive datasets, enabling scalable big data solutions.
* 2008: The term **data scientist** gained traction at LinkedIn (*D.J. Patil*) and Facebook (*Jeff Hammerbacher*). <font color = "blue">*\color{blue} Patil later served as the Chief Data Scientist of the United States Office of Science and Technology Policy from 2015 to 2017.*</font>
* 2011: Data science job postings surged by 15,000%, reflecting industry demand.
* 2012: Harvard Business Review dubbed data scientist the **sexiest job of the 21st century**.
* 2015:
  + TensorFlow (Google) and deep learning accelerated AI adoption.
  + <font color = "red">*\color{red} Machine learning became central to data science workflows*</font>.
  
**Modern Era (2018–Present)**

* 2018: Ethical AI gained prominence with studies like Gender Shades, exposing bias in facial recognition.
* 2020: COVID-19 spurred data-driven solutions, such as the WHO’s Solidarity Trial.
* 2025:
  + Big data analytics in banking is projected to reach $62 billion.
  + Global data creation is expected to exceed 180 zettabytes <font color = "red">(1 zettabyte = a trillion gigabytes, or $10^{12}$ GB)</font>.
  
  
  
**Critical Shifts and Innovations**

* From Statistics to Computation:
  + Early statistical foundations (Tukey, Naur) evolved with tools like Python, Hadoop, and cloud computing.
  + AutoML and NoSQL streamlined data processing.
  
* AI and Machine Learning Integration:
  + Neural networks (1980s) and deep learning (2010s) transformed predictive analytics.
  + AlphaZero (2017) demonstrated AI’s superhuman problem-solving.
  
* Ethical and Regulatory Focus:
  + GDPR (2018) and algorithmic fairness research redefined data governance.
  
* Future Trajectories
  + Generative AI: Tools like ChatGPT are reshaping data analysis and automation.
  + Edge Computing: IoT and real-time analytics drive decentralized data processing.
  + Interdisciplinary Collaboration: Domain expertise (e.g., healthcare, finance) is increasingly integrated into data pipelines.


By understanding these milestones, educators and practitioners can better navigate the evolving landscape, balancing statistical rigor with computational innovation.



# Relation to Neighboring Disciplines



Data science is closely related to three technical fields: (computational) mathematics, applied statistics, and computer science (programming, database and software development skills). These three fields have distinct foundations, goals, and methodologies. 


| 	 |Modern Data Science	| Classical Statistics	| Classical Computer Science |
|:------------|:-------------------|:----------------------|:---------------------------|
| **Primary Goal**	| Extract insights and predictions from data.	| Analyze data to infer patterns and relationships.	| Solve computational problems efficiently.| 
| **Core Focus**	| Data analysis, machine learning, and visualization.	| Parametric and non parametric inference, and study design.	| Algorithms, system design, and software development.| 
| **Mathematical Basis**	| Calculus, linear algebra, numerical optimization.	| Probability theory, mathematical statistics.	| Discrete mathematics, logic, computability.| 
| **Tools and Technologies**	| Python, R, SQL, Tableau, TensorFlow, Spark.	| R(primarily used as a package), SAS (Statistical Analysis System), SPSS(Statistical package for Social Science), Excel.	| Java, C++, Git, Linux, SQL.| 
| **Data Handling**	| Structured and unstructured data, big data.	| Structured data, small to medium datasets.	| Structured data, system-level data.| 
| **Output**	| Insights, models, dashboards, predictions.	| Reports, p-values, confidence intervals.	| Software, systems, algorithms, hardware.| 
| **Applications**	| Business analytics, AI, recommendation systems.	| Research, quality control, public policy.	| Software development, cybersecurity, networking.| 



```{r echo = FALSE, fig.align='center', out.width="60%"}
include_graphics("DS-Vendiagram.png")
```


**Data science** is often considered a **new discipline** that emerged at the intersection of statistics, computer science, and domain expertise. It integrates techniques and methodologies from these fields to extract insights and value from data. Similarly, machine learning and data engineering are closely related subfields of computer science that have evolved alongside data science, each with its own distinct focus and role in the data ecosystem. 

**Machine learning (ML)** is widely regarded as the technical foundation of both **data science** and **artificial intelligence (AI)**. **Data Science** uses **machine learning** to analyze data and solve specific problems. **AI** uses machine learning to create systems that can perform tasks requiring human-like intelligence.

**Data engineering** is a subfield of computer science that plays a critical role in enabling and supporting data science. While **data science** focuses on extracting insights and building models from data, **data engineering** pprovides the technical foundation for data science. It focuses on building and maintaining the infrastructure needed to collect, store, process, and integrate data.

To summarize, the following gives a breakdown comparison.


| 	 |Data Science	| Machine Learning	| Data Engineering |
|:------------|:-------------------|:----------------------|:---------------------------|
| **Primary Goal**	| Extract insights and predictions from data.	| Build systems that learn from data.	| Build and maintain data infrastructure.| 
| **Core Focus**	| Data analysis, modeling, and visualization.	| Algorithm development and model training.	| Data pipelines, databases, and big data tools.| 
| **Key Skills**	| Statistics, programming, domain knowledge.	| Algorithms, optimization, model evaluation.	| ETL/ELT, database design, cloud platforms.| 
| **Tools**	| Python, R, Tableau, Spark.	| TensorFlow, PyTorch, Scikit-learn.	| Hadoop, Spark, SQL, AWS, Azure.| 
| **Output**	| Insights, models, dashboards.	| Predictive models, decision-making systems.	| Data pipelines, databases, warehouses.| 
| **Applications**	| Business analytics, healthcare, marketing.	| Recommendation systems, NLP, image recognition.	| Real-time analytics, data warehousing.| 


The following disgram depicts the relationship between related disciplines and fields in these disciplines.

```{r echo = FALSE, fig.align='center', out.width="65%"}
include_graphics("DS-diagram.png")
```



# Relation to Classical Statistics

Classical statistics and data science overlap in many ways but have different foundational principles that shape their approaches, methodologies, and applications. While statistics provides the mathematical and inferential backbone for data analysis, data science extends beyond traditional statistical methods by integrating computational techniques, machine learning, and big data processing.

## Breakdown Comparisons

We next provide tabular comparisons


**1. Core Philosophical Foundations**

The two fields have different philosophical foundations. Classical statistics prioritizes inference and explainability, while data science focuses on pattern discovery and prediction. 


|  	| Classical Statistics |	Data Science |
|:--------|:---------------------|:---------------------|
|**Core Philosophy**|	Understanding relationships, making inferences from data |	Discovering patterns, making predictions, and optimizing decisions |
|**Approach**	| Deductive (theory-driven)	| Inductive (data-driven) |
|**Primary Goal** |	Hypothesis testing, inference, estimation |	Prediction, automation, pattern recognition |
|**Uncertainty Handling** |	Probability theory, confidence intervals, significance testing |	Probabilistic models, uncertainty quantification in ML|
|**Causal Inference** |	Strong emphasis (experiments, counterfactuals) |	Less emphasis (correlation-based, emerging causal AI methods) |


**2. Mathematical Foundations**

The two fields have partially overlapping foundations. Classical statistics relies more on probability and inference, while data science incorporates additional mathematical fields like optimization and discrete math.


|  	| Classical Statistics |	Data Science |
|:--------|:---------------------|:---------------------|
|**Probability Theory** |	Central to hypothesis testing and estimation	|Used but often combined with empirical modeling |
|**Linear Algebra**	|Used in regression and multivariate analysis	|Critical for machine learning, deep learning|
|**Calculus**	| Used in likelihood estimation and probability distributions	|Essential for optimization algorithms in ML|
|**Optimization**	|Constrained optimization (MLE, least squares)|	Stochastic gradient descent, reinforcement learning|
|**Discrete**| Math & Graph Theory	|Rarely used	Essential for algorithms, networks, graph-based learning|



**3. Methodological Foundations**

The two fields also have different methodological foundations. Classical statistics relies on explicit parametric models and interpretability, while data science is more flexible with nonparametric, automated models.


|  	| Classical Statistics |	Data Science |
|:--------|:---------------------|:---------------------|
| **Modeling Paradigm**	| Parametric models (e.g., linear regression, ANOVA)	| Nonparametric models (e.g., decision trees, deep learning)| 
| **Estimation Methods**| 	Frequentist (MLE, least squares), Bayesian inference	| Machine learning-based optimization (SGD, backpropagation)| 
| **Validation Techniques**| 	p-values, confidence intervals, residual analysis	| Cross-validation, bootstrapping, train-test splits| 
| **Overfitting Control**| 	Regularization (Ridge, Lasso), adjusted R²	| Dropout, ensemble learning, early stopping| 
| **Feature Selection**	| PCA, variable selection	| Automated feature extraction (deep learning, embeddings)| 


**4. Data Handling & Processing Foundations**

The two different data handling and processing foundations. Classical statistics assumes structured, well-behaved data, while data science handles diverse, large-scale, and unstructured data.

|  	| Classical Statistics |	Data Science |
|:--------|:---------------------|:---------------------|
| **Data Size**	| Works best with small to moderate datasets	| Designed for big data, high-dimensional data| 
| **Data Assumptions**	| Normality, independence, linearity	| Assumption-free, adaptable to noisy data| 
| **Handling Missing Data**	| Imputation via mean, regression, MICE	| Advanced imputation using deep learning, GANs| 
| **Dimensionality** | Handling	PCA, factor analysis	| High-dimensional feature learning (deep embeddings, autoencoders)| 



**5. Computational Foundations**

The two different computational foundations. Classicall statistics relies on analytical methods, while data science embraces computational and algorithmic complexity.


|  	| Classical Statistics |	Data Science |
|:--------|:---------------------|:---------------------|
| **Computational Complexity**	| Low (analytical solutions preferred)	| High (requires GPUs, cloud computing)| 
| **Algorithm Implementation**	| Algebraic solutions, matrix operations	| Iterative, algorithmic methods (gradient descent)| 
| **Software Tools**	| R, SAS, SPSS	| Python (TensorFlow, PyTorch, Scikit-Learn), Spark| 
| **Scalability**	| Manual model tuning	| AutoML, distributed computing| 



**6. Interpretability vs. Predictive Power**

The two fields have different modeling goals. Classical statistics prioritizes explainability, while data science prioritizes accuracy and automation.


|  	| Classical Statistics |	Data Science |
|:--------|:---------------------|:---------------------|
| **Interpretability**	| High—emphasis on explainable models	| Often low—deep learning and black-box models| 
| **Prediction vs. Inference**	| Focuses on inference and causality	| Focuses on prediction and generalization| 
| **Explainability Tools**	| Regression coefficients, confidence intervals	| SHAP, LIME, feature importance| 




**7. Applications Across Disciplines**

The two fields are complementary foundations. Classical statistics is crucial for regulated environments, while data science excels in AI-driven applications.


|  	| Classical Statistics |	Data Science |
|:--------|:---------------------|:---------------------|
| **Healthcare**	| Clinical trials, survival analysis	| AI-assisted diagnosis, medical imaging| 
| **Finance**	| Risk modeling, econometrics	| Algorithmic trading, fraud detection| 
| **Marketing**	| A/B testing, customer segmentation	| Recommendation systems, sentiment analysis| 
| **Engineering**	| Reliability analysis, quality control	| Predictive maintenance, AI-driven design| 



## Differences and Overlaps

Classical statistics and data science have different but complementary foundations. A strong statistical background enhances data science applications, and data science expands statistical analysis with computational power and automation.

### Key Differences

* Statistics is rooted in mathematical theory, probability, and inference, focusing on explainability, hypothesis testing, and uncertainty quantification.

* Data Science is rooted in computation, machine learning, and big data processing, prioritizing prediction, pattern discovery, and automation.

### Key Overlaps

* Both use probability, linear algebra, and statistical modeling to analyze data.

* Many machine learning methods originate from statistical techniques (e.g., regression, Bayesian inference).

* Modern data science integrates statistical reasoning to enhance model robustness and interpretation.

\

# Visual Analytics

Visual Analytics is an interdisciplinary field that combines techniques from data analytics, information visualization, human-computer interaction, and cognitive science to enable the exploration, analysis, and communication of complex data. 

The key foundations of visual analytics are rooted in the integration of multiple disciplines, each contributing essential components to the field. These foundations enable the effective exploration, analysis, and communication of complex data.


* **Data Analytics and Statistics**
  + Provides the methods for processing, analyzing, and interpreting data to extract meaningful insights.
  + Forms the backbone of data-driven decision-making by enabling quantitative analysis and predictive modeling.
  
* **Information Visualization**
  + Focuses on designing visual representations of data (e.g., charts, graphs, maps) to make complex information more accessible and understandable.
  + Enhances data comprehension by translating abstract numbers and relationships into intuitive visual formats.

* **Human-Computer Interaction (HCI)** 
  + Ensures that visualization tools are user-friendly, intuitive, and tailored to support human cognitive processes.
  + Bridges the gap between humans and machines by designing interfaces that facilitate seamless exploration and interaction with data.

* **Cognitive Science**
  + Informs how humans perceive, interpret, and reason about visual information, enabling the design of effective visual interfaces.
  + Informs the design of visualizations that align with human cognitive abilities, ensuring that insights are effectively communicated and understood.

* **Computer Science**
  + Supplies the algorithms, computational power, and frameworks necessary to handle large-scale data processing and real-time interactivity.
  +  Enables the development of scalable and efficient tools for data analysis and visualization.

* **Domain Knowledge**
  + Involves expertise in specific fields (e.g., healthcare, finance, climate science) to ensure that visual analytics tools address relevant problems and provide actionable insights.
  + Ensures that visualizations and analyses are contextually meaningful and aligned with the needs of the target audience.


* **Visual Design Principles**
  + Applies principles of design (e.g., color theory, layout, typography) to create visually appealing and effective representations of data.
  + Ensures that visualizations are not only informative but also engaging and easy to interpret.

* **Machine Learning and AI**
  + Enhances data analysis by automating pattern recognition, anomaly detection, and predictive modeling.
  + Complements human reasoning by identifying complex patterns in data that may not be immediately apparent.



# Ambiguous Termiologies

The ambiguity of terminologies used in data science and related disciplines is another significant contributor to misconceptions in the field.

## Polysemous Terminology

In data science and statistics, certain terms share identical names but carry entirely different meanings. This overlap often leads to the misconception that data science and statistics are the same. Below is a list of some of these ambiguous terms.

**1. Bias**

* **In Statistics**: The systematic error in an estimator that causes it to deviate from the true parameter (e.g., bias in an estimator like underestimating the population mean).

* **In Machine Learning**: Bias refers to the tendency of a model to make consistent errors, often in the context of the bias-variance trade-off (e.g., high bias leads to underfitting).

**2. Kernel**

* **In Statistics**: A weighting function used in kernel density estimation (KDE) and nonparametric regression.

* **In Machine Learning**: A mathematical function that implicitly maps data into a higher-dimensional space, enabling the kernel trick in algorithms like Support Vector Machines (SVM) and Kernel PCA.

**3. Confidence Interval vs. Prediction Interval**

* **In Statistics**: A confidence interval estimates the range where a population parameter (e.g., mean) is likely to fall.
* **In Data Science (Machine Learning)**: A prediction interval gives the range where a new observation is likely to fall, typically wider than a confidence interval.

**4. Regularization**

* **In Statistics**: A technique to prevent overfitting by adding constraints or penalties to regression models (e.g., Ridge and Lasso regression).

* **In Deep Learning**: Regularization includes dropout, batch normalization, and early stopping, which help prevent neural networks from memorizing training data.

**5. Feature**

* **In Statistics**: Often refers to explanatory variables (independent variables in regression models).

* **In Machine Learning**: A feature is any measurable input used by a model, including engineered features created from raw data.


**6. Overfitting**

* **In Statistics**: Not commonly used in traditional statistics but relates to overparameterization, where a model has too many parameters and fits noise instead of the true pattern.

* **In Machine Learning**: Overfitting occurs when a model learns training data too well, capturing noise instead of general patterns, leading to poor performance on unseen data.

**7. Sampling**

* **In Statistics**: Sampling refers to selecting a subset of data from a population for analysis (e.g., random sampling, stratified sampling).

* **In Machine Learning**: Sampling can refer to techniques like data augmentation, bootstrapping, or sampling from a probability distribution (e.g., sampling latent variables in a Variational Autoencoder).

**8. Normalization**

* **In Statistics**: Refers to transforming data to have a specific distribution, such as standardization (subtracting the mean and dividing by the standard deviation).

* **In Machine Learning**: Often means scaling inputs to a <font color = "red">*\color{red}specific range (e.g., 0 to 1)*</font> to improve model performance.


## Terminological Variants


There are also cases where different terms are used in statistics and data science, but they mean the same thing. Here are some key examples.

* **Independent Variable vs Feature** 
  + **In statistics**, an independent variable is an explanatory variable used in regression or other models.
  + **In data science**, a feature is any input variable used to train a model.

* **Dependent Variable vs Target/Label**
  + **In statistics**, the dependent variable is the outcome being predicted (e.g., in regression).
  + **In data science**, it is often called the target or label, especially in supervised learning.
  
* **Predictor vs Input Feature**
  + **In statistics**, a predictor is an independent variable used in regression models.
  * **In machine learning**, this is simply called a feature.
  
* **Coefficient vs Weight**
  + **In statistics**, a coefficient represents the estimated effect of an independent variable in regression models.
  * **In machine learning**, the term weight is often used instead, especially in neural networks and linear models.
  
* **Error Term vs Noise**
  + **In statistics**, an error term accounts for variability not explained by a model.
  + **In machine learning**, this is often referred to as noise, representing unpredictable or random variations in data.
  
* **Multicollinearity vs Feature Dependence**
  + **In statistics**, multicollinearity occurs when independent variables are highly correlated, leading to instability in regression models.
  + **In machine learning**, this issue is often called feature dependence or redundancy, and methods like principal component analysis (PCA) or feature selection are used to address it.
  
* **Interaction Term vs Feature Engineering**
  + **In statistics**, an interaction term is created to model the combined effect of two or more variables (e.g., in regression models).
  + **In data science**, this is part of feature engineering, where new features are created by combining or transforming existing ones.
  
* **Likelihood Function vs Log-Loss**
  + **In statistics**, the likelihood function is used to estimate parameters (e.g., in Maximum Likelihood Estimation).
  + **In machine learning**, a similar concept is log-loss, used to optimize classification models.

* **Shrinkage vs Regularization**
  + **In statistics**, shrinkage refers to techniques that reduce coefficient values to prevent overfitting (e.g., Ridge regression).
  + **In machine learning**, this is called regularization and includes methods like L1 (Lasso) and L2 (Ridge) regularization.
  
* **Bootstrap Sampling vs Data Resampling**
  + **In statistics**, bootstrap sampling is used to estimate uncertainty by resampling from the dataset with replacement.
  + **In machine learning**, resampling techniques like bagging (Bootstrap Aggregating) use the same concept to improve model stability.
















